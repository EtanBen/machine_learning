# TODO: Import necessary libraries
import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import load_digits
from sklearn import decomposition
import pandas as pd
import seaborn as sns
from sklearn.decomposition import PCA
from skimage.metrics import structural_similarity as ssim   # pip install scikit-image

##########################################
## Data loading and first visualisation
##########################################

# Load the handwritten digits dataset
#digits = # TODO: Write your own code!
digits = load_digits()
print(digits.data.shape)

plt.show()


# Visualize some images
# TODO: Graph the first4 images from the data base 
""""
for i in range (4):
    plt.matshow(digits.images[i], cmap="gray")
"""

# Display at least one random sample par class (some repetitions of class... oh well)
def plot_multi(data, y):
    '''Plots 16 digits'''
    nplots = 16
    nb_classes = len(np.unique(y))
    cur_class = 0
    fig = plt.figure(figsize=(15,15))
    for j in range(nplots):
        plt.subplot(4,4,j+1)
        to_display_idx = np.random.choice(np.where(y == cur_class)[0])
        plt.imshow(data[to_display_idx].reshape((8,8)), cmap='binary')
        plt.title(cur_class)
        plt.axis('off')
        cur_class = (cur_class + 1) % nb_classes
    plt.show()


plot_multi(digits.data, digits.target)

##########################################
## Data exploration and first analysis
##########################################

def get_statistics_text(targets):
    # TODO: Write your code here, returning at least the following useful infos:
    # * Label names
    # * Number of elements per class
    df=pd.DataFrame({'Classe': targets})
    compter=df['Classe'].value_counts().sort_index()
    print("distribution")
    print(compter)
    
    return compter


def compare_img(img1, img2):
    """
    Compare deux images en utilisant le SSIM et affiche les résultats.
    
    Args:
        img1: Première image (numpy array)
        img2: Deuxième image (numpy array)
    
    Returns:
        float: Score SSIM entre les deux images
    """
  
    

    




    
    score, diff = ssim(img1, img2, full=True,data_range=2)
    
    print(f"SSIM Score: {score:.4f}")

    # Affichage
    """
    plt.figure(figsize=(15, 5))
    
    plt.subplot(1, 3, 1)
    plt.imshow(img1_display, cmap=None if color else 'gray')
    plt.title("Image 1")
    plt.axis('off')

    plt.subplot(1, 3, 2)
    plt.imshow(img2_display, cmap=None if color else 'gray')
    plt.title("Image 2")
    plt.axis('off')

    plt.subplot(1, 3, 3)
    plt.imshow(diff, cmap='gray')
    plt.title(f"Différence (SSIM: {score:.4f}")
    plt.axis('off')

    plt.tight_layout()
    plt.show()
    """
    
    return score



# TODO: Call the previous function and generate graphs and prints for exploring and visualising the database
stats=get_statistics_text(digits.target)

sns.barplot(x=stats.index, y=stats.values)
plt.show()


##########################################
## Start data preprocessing
##########################################

# Access the whole dataset as a matrix where each row is an individual (an image in our case) 
# and each column is a feature (a pixel intensity in our case)
## X = [
#  [Pixel1, Pixel2, ..., Pixel64],  # Image 1 as a row
#  [Pixel1, Pixel2, ..., Pixel64],  # Image 2 as a row
#  [Pixel1, Pixel2, ..., Pixel64],  # Image 3 as a row
#  [Pixel1, Pixel2, ..., Pixel64]   # Image 4 as a row
#]

# TODO: Create a feature matrix and a vector of labels
X = digits.data # matrice N*m avec N le nb ind et m le nb features
y = digits.target

#print(X)
# Print dataset shape
print(f"Feature matrix shape: {X.shape}. Max value = {np.max(X)}, Min value = {np.min(X)}, Mean value = {np.mean(X)}")
print(f"Labels shape: {y.shape}")


# TODO: Normalize pixel values to range [0,1]
F = X/16  # Feature matrix after scaling : on divise par 16
#F : chaque ligne = une image (normalisé)

# Print matrix shape
print(f"Feature matrix F shape: {F.shape}. Max value = {np.max(F)}, Min value = {np.min(F)}, Mean value = {np.mean(F)}")

##########################################
## Dimensionality reduction
##########################################


### just an example to test, for various number of PCs
sample_index = 0
original_image = F[sample_index].reshape(8, 8)  # Reshape back to 8×8 for visualization (image à reproduire)

# TODO: Using the specific sample above, iterate the following:
# * Generate a PCA model with a certain value of principal components (1)
# * Compute the approximation of the sample with this PCA model (2)
# * Reconstruct a 64 dimensional vector from the reduced dimensional PCA space
# * Reshape the resulting approximation as an 8x8 matrix
# * Quantify the error in the approximation
# Finally: plot the original image and the 15 approximation on a 4x4 subfigure


#(1) avec 2 composantes
C=np.transpose(F)@F #X'*X = PDP' avec P mat des vect p, D diag des val p
print(C)

pca= PCA(n_components=2)
principalComponents = pca.fit(F) #décompo en val singulières de la matrice de données
principalComponents2=pca.transform(F[sample_index:sample_index+1]) #produit scalaire (projection)
#applique P en produit scal (ex : si principalComponents est 8*8, on a 8 ind de 8 features)

approx = pca.inverse_transform(principalComponents2)
approx_image=approx.reshape(8,8)

erreur=np.linalg.norm(original_image-approx_image)/np.linalg.norm(original_image)
print("erreur :",erreur)



compare_img(original_image,approx_image)

fig, axes = plt.subplots(4, 4, figsize=(8, 8))
axes=axes.flatten

#boucle : plot the original image and the 15 approximation on a 4x4 subfigure
for k in range (16):
    
    pca= PCA(n_components=k)
    principalComponents = pca.fit(F)
    principalComponents2 = pca.transform(F[sample_index:sample_index+1])

    approx = pca.inverse_transform(principalComponents2)
    approx_image=approx.reshape(8,8)
    
    erreur=np.linalg.norm(original_image-approx_image)/np.linalg.norm(original_image)
    SSIM= compare_img(original_image,approx_image)
    print("erreur :",erreur)
    print("PCA:",principalComponents2)

    
    plt.subplot(4, 4, k+1)
    plt.imshow(approx_image, cmap='gray')
    plt.title(f"k={k},SSIM={SSIM}")


plt.tight_layout()
plt.show()



#### TODO: Expolore the explanined variance of PCA and plot  

pca2= PCA().fit(F)

var_cumulee=np.cumsum(pca2.explained_variance_ratio_)


plt.plot(var_cumulee)
plt.xlabel("Nb de composantes")
plt.ylabel("Variance expliquée cumulée")
plt.title("Variance expliquée selon le nombre de composantes")
plt.show()


# Question : Que faire si on utilise pas l'acp
#Comment générer des images à partir de l'acp

# Create the visualization plot


### TODO: Display the whole database in 2D: 

principal_components = pca.fit_transform(F)

sns.scatterplot(x=principal_components[:, 0], y=principal_components[:, 1], hue=digits.target,palette='bright')
plt.title('Database 2D')
plt.xlabel('1 ère composante ')
plt.ylabel('2ème composante')
plt.legend(title='Chiffre')
plt.show()

### TODO: Create a 20 dimensional PCA-based feature matrix

pca20= PCA(n_components=20)

F_pca = pca20.fit_transform(F)

# Print reduced feature matrix shape
print(f"Feature matrix F_pca shape: {F_pca.shape}")



##########################################
## Feature engineering
##########################################
### # Function to extract zone-based features
###  Zone-Based Partitioning is a feature extraction method
### that helps break down an image into smaller meaningful regions to analyze specific patterns.
def extract_zone_features(images):
    '''Break down an 8x8 image in 3 zones: row 1-3, 4-5, and 6-8'''
    # TODO: Fill in code
    features = []
    for image in images:
        img = image.reshape(8, 8)
        zone1 = np.mean(img[0:3, :])  
        zone2 = np.mean(img[3:5, :])  
        zone3 = np.mean(img[5:8, :]) 
        features.append([zone1, zone2, zone3])
    return np.array(features) # regader l'intensité des bandes 

# Apply zone-based feature extraction
F_zones = extract_zone_features(F)

# Print extracted feature shape
print(f"Feature matrix F_zones shape: {F_zones.shape}")


### Edge detection features

from scipy.ndimage import sobel
A = sobel(original_image,axis=-1,output=None,cval=0.0,mode='reflect')
plt.imshow(A,cmap='gray')

## TODO: Get used to the Sobel filter by applying it to an image and displaying both the original image 
# and the result of applying the Sobel filter side by side


# TODO: Compute the average edge intensity for each image and return it as an n by 1 array
F_edges = None

# Print feature shape after edge extraction
print(f"Feature matrix F_edges shape: {F_edges.shape}")
# PCA feature 
pca = PCA().fit(F)  # F : matrice normalisée (ex: digits.data / 16)
#calculer autant de composantes qu’il y a de dimensions dans F
#entraîne ce modèle sur les données F 
# Variance expliquée cumulée
cum_var = np.cumsum(pca.explained_variance_ratio_)

plt.figure(figsize=(8,4))
plt.plot(cum_var, marker='o')
plt.axhline(y=0.99, color='r', linestyle='--', label='95% de variance')
plt.xlabel("Nombre de composantes principales")
plt.ylabel("Variance cumulée expliquée")
plt.title("Choix du nombre optimal de composantes PCA")
plt.grid(True)
plt.legend()
plt.show()
n_components = np.argmax(cum_var >= 0.95) 
print(f"Nombre minimal de composantes pour 95% de variance : {n_components}")

### connect all the features together

# TODO: Concatenate PCA, zone-based, and edge features
F_final = None 

# TODO: Normalize final features
F_final = F_final

# Print final feature matrix shape
print(f"Final feature matrix F_final shape: {F_final.shape}")



# A LA FIN : génerer une matrice de taille 24 col * nb échantillons 
